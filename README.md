![Banner](./chatbot.png)
# ğŸ’¬ LangGraph Memory Chatbot

<div align="center">

**A production-ready conversational AI backend built with LangGraph, featuring dual memory modes, tool integration, and a beautiful Streamlit UI**

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![LangChain](https://img.shields.io/badge/LangChain-0.3+-green.svg)](https://www.langchain.com/)
[![LangGraph](https://img.shields.io/badge/LangGraph-0.2+-orange.svg)](https://github.com/langchain-ai/langgraph)
[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4o--mini-purple.svg)](https://openai.com/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-teal.svg)](https://fastapi.tiangolo.com/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.50+-red.svg)](https://streamlit.io/)

*Perfect for learning LangGraph, conversational AI, and building production-ready chatbots*

</div>

---

## ğŸ¯ What is This Project?

This is a **complete conversational AI application** that demonstrates how to build an intelligent chatbot system that:

- Maintains conversation context with dual memory modes
- Integrates external tools (Wikipedia, Weather)
- Uses LangGraph for sophisticated workflow orchestration
- Provides both REST API and web UI interfaces

**Perfect for students learning:**

- ğŸ¤– LangGraph workflow orchestration
- ğŸ”— LangChain framework integration
- ğŸ’¾ Memory management (temporary vs persistent)
- ğŸ› ï¸ Tool integration and function calling
- ğŸŒ Building full-stack AI applications
- ğŸš€ Production-ready chatbot architecture

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ”„ **Dual Memory Modes** | Temporary (in-memory) and Persistent (SQLite-backed) conversation history |
| ğŸ› ï¸ **Tool Integration** | Wikipedia summaries and weather information via slash commands |
| ğŸ§  **Smart Context Management** | Automatic message trimming (1200 tokens) to prevent context overflow |
| ğŸš€ **FastAPI Backend** | RESTful API with Pydantic validation and session management |
| ğŸ¨ **Streamlit UI** | Clean, intuitive chat interface with session management |
| ğŸ“Š **LangGraph Workflow** | Sophisticated state machine for conversation flow |
| ğŸ”’ **Production Ready** | Modular architecture, error handling, and best practices |

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit UI   â”‚  User types: "Hello!" or "/wiki Python"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP POST
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      FastAPI Backend                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  POST /chat                   â”‚  â”‚
â”‚  â”‚  - session_id                 â”‚  â”‚
â”‚  â”‚  - message                    â”‚  â”‚
â”‚  â”‚  - memory mode                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      LangGraph Workflow             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. respond_node              â”‚  â”‚
â”‚  â”‚     - Process user input      â”‚  â”‚
â”‚  â”‚     - Trim message history    â”‚  â”‚
â”‚  â”‚     - Call LLM                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚                    â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚         â”‚ needs_tools?   â”‚           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                 â”‚                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚    â”‚                         â”‚       â”‚
â”‚    â–¼                         â–¼       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ tools    â”‚          â”‚   END    â”‚ â”‚
â”‚  â”‚  node    â”‚          â”‚          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚                              â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚                  â–¼                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚         â”‚  Return to      â”‚          â”‚
â”‚         â”‚  respond_node   â”‚          â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Memory Store  â”‚
         â”‚  (Temporary or â”‚
         â”‚   Persistent)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack

<div align="center">

| Category | Technology | Purpose |
|----------|-----------|---------|
| **ğŸ¤– AI/ML** | LangChain | Core LLM framework |
| | LangGraph | Workflow orchestration and state management |
| | OpenAI GPT-4o-mini | LLM for conversation and tool calling |
| **ğŸŒ Backend** | FastAPI | REST API server |
| **ğŸ’» Frontend** | Streamlit | Interactive web interface |
| **ğŸ’¾ Storage** | SQLite | Persistent conversation storage |
| | MemorySaver | In-memory temporary storage |
| **âš™ï¸ Tools** | uv | Fast Python package manager |
| | Python 3.10+ | Programming language |

</div>

---

## ğŸ“¦ Project Structure

```
langgraph-memory-chatbot/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # âš™ï¸ Configuration & environment variables
â”‚   â”œâ”€â”€ schema.py              # ğŸ“‹ Pydantic models (ChatRequest, ChatResponse)
â”‚   â”œâ”€â”€ graph.py               # ğŸ§  LangGraph workflow definition
â”‚   â”œâ”€â”€ main.py                # ğŸš€ FastAPI application
â”‚   â””â”€â”€ ui.py                  # ğŸ¨ Streamlit interface
â”‚
â”œâ”€â”€ .data/                     # ğŸ“ Data directory (created automatically)
â”‚   â””â”€â”€ memory.sqlite          # ğŸ’¾ Persistent conversation storage
â”‚
â”œâ”€â”€ pyproject.toml             # ğŸ“‹ Dependencies & project config
â”œâ”€â”€ uv.lock                    # ğŸ”’ Dependency lock file
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+** installed
- **OpenAI API Key** ([Get one here](https://platform.openai.com/api-keys))
- **uv** package manager (we'll install it if needed)

### Installation Steps

#### 1ï¸âƒ£ Install uv (if needed)

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

#### 2ï¸âƒ£ Clone and Navigate

```bash
git clone https://github.com/JaimeLucena/langgraph-memory-chatbot.git
cd langgraph-memory-chatbot
```

#### 3ï¸âƒ£ Install Dependencies

```bash
uv sync
```

This will create a virtual environment and install all required packages.

#### 4ï¸âƒ£ Configure Environment

Create a `.env` file in the root directory:

```bash
OPENAI_API_KEY=sk-your-api-key-here
OPENAI_MODEL=gpt-4o-mini
SQLITE_PATH=.data/memory.sqlite
SYSTEM_PROMPT=You are a helpful and concise assistant.
```

> ğŸ’¡ **Tip**: Never commit your `.env` file! It's already in `.gitignore`

#### 5ï¸âƒ£ Create Data Directory

```bash
mkdir -p .data
```

---

## ğŸ® Usage

### 1ï¸âƒ£ Start the Backend

First, start the FastAPI backend server:

```bash
uv run uvicorn app.main:app --reload --port 8000
```

The API will be available at **http://localhost:8000**

> ğŸ’¡ **Tip**: Keep this terminal open. The backend must be running for the frontend to work.

### 2ï¸âƒ£ Start the Frontend (Streamlit UI)

In a **separate terminal**, launch the Streamlit web interface:

```bash
uv run streamlit run app/ui.py
```

The UI will open automatically in your browser at **http://localhost:8501**

**Features:**

- ğŸ’¬ Clean chat interface for conversations
- ğŸ”’ **Persistent chats**: Saved conversations with SQLite backend
- âš¡ **Temporary chats**: In-memory conversations (lost on refresh)
- ğŸ› ï¸ **Tool commands**: Use `/wiki <topic>` or `/weather <city>`
- ğŸ“ **Chat history**: View and manage multiple conversation sessions
- ğŸ¨ **Modern UI**: Intuitive interface with session management

### 3ï¸âƒ£ Using the REST API Directly (Optional)

If you prefer to interact with the API directly without the UI, you can use the REST endpoints.

**Interactive API Docs:** Visit **http://localhost:8000/docs** for Swagger UI

#### API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/chat` | Send a message and receive a response |

#### Example API Request

```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "user-123",
    "message": "Hello! How are you?",
    "memory": "persistent"
  }'
```

**Response:**

```json
{
  "session_id": "user-123",
  "reply": "I'm doing well, thank you! How can I help you today?",
  "mode": "persistent",
  "tokens_input": null,
  "tokens_output": null
}
```

---

## ğŸ’¡ Example Interactions

Try these interactions to see the chatbot in action:

### Basic Conversations

- `"Hello! How are you?"`
- `"What can you help me with?"`
- `"Tell me a joke"`
- `"Explain quantum computing in simple terms"`

### Tool Commands

#### Wikipedia Tool

- `/wiki Python programming`
- `/wiki Albert Einstein`
- `/wiki Machine Learning`

#### Weather Tool

- `/weather Madrid`
- `/weather New York`
- `/weather Tokyo`

### Memory Testing

1. **Start a persistent chat** and ask: `"My name is Alice"`
2. **In the same session**, ask: `"What's my name?"`
3. The bot should remember: `"Your name is Alice"`

4. **Start a temporary chat** and have a conversation
5. **Refresh the page** - the temporary chat is lost
6. **Persistent chats** survive page refreshes and restarts

---

## ğŸ§  How LangGraph Works Here

### Step-by-Step Process

1. **User Input** â†’ Message sent to FastAPI endpoint

   ```
   POST /chat
   {
     "session_id": "user-123",
     "message": "/wiki Python",
     "memory": "persistent"
   }
   ```

2. **Graph Invocation** â†’ LangGraph processes the message

   ```
   - Load conversation history from checkpointer (if persistent)
   - Trim messages to fit token limit (1200 tokens)
   - Route to respond_node
   ```

3. **LLM Processing** â†’ Model decides on tool usage

   ```
   - If message starts with "/wiki" or "/weather" â†’ bind tools
   - Otherwise â†’ regular conversation
   - Generate response or tool calls
   ```

4. **Tool Execution** (if needed)

   ```
   - Execute wiki_summary("Python") or get_weather("city")
   - Return tool results to LLM
   - LLM formats final answer
   ```

5. **State Persistence** â†’ Save conversation state

   ```
   - Temporary: In-memory (MemorySaver)
   - Persistent: SQLite database
   ```

6. **Response** â†’ Return formatted answer to user

   ```
   {
     "reply": "Python is a high-level programming language...",
     "mode": "persistent"
   }
   ```

### Key Components

- **`app/graph.py`**: Core LangGraph workflow
  - `ChatState`: TypedDict for conversation state
  - `respond_node`: Main LLM interaction node
  - `tools_node`: Tool execution node
  - `needs_tools`: Routing logic for tool usage
  - Message trimming to prevent context overflow

- **`app/main.py`**: FastAPI application
  - Dual graph instances (temporary/persistent)
  - Session-based conversation management
  - SQLite checkpointer lifecycle

- **`app/schema.py`**: Pydantic models
  - `ChatRequest`: Input validation
  - `ChatResponse`: Output formatting
  - `MemoryMode`: Type-safe memory mode selection

---

## ğŸ“Š Memory Modes Explained

### Temporary Memory (MemorySaver)

- **Storage**: In-memory (RAM)
- **Persistence**: Lost on server restart
- **Use Case**: Quick, ephemeral conversations
- **Performance**: Faster (no disk I/O)
- **Session**: Each `session_id` maintains separate state

### Persistent Memory (SqliteSaver)

- **Storage**: SQLite database file
- **Persistence**: Survives server restarts
- **Use Case**: Long-term conversations, user history
- **Performance**: Slightly slower (disk I/O)
- **Session**: Each `session_id` maintains separate state across restarts

### Choosing the Right Mode

| Scenario | Recommended Mode |
|----------|-----------------|
| Quick questions | `temporary` |
| Long conversations | `persistent` |
| User accounts | `persistent` |
| Testing/development | `temporary` |
| Production | `persistent` |

---

## ğŸ“ Learning Objectives

By exploring this project, you'll learn:

âœ… **LangGraph Fundamentals**

- Building state machines for conversations
- Node-based workflow orchestration
- Conditional routing and decision making
- State persistence and checkpoints

âœ… **Memory Management**

- Temporary vs persistent storage
- Conversation state management
- Session-based isolation
- Message history trimming

âœ… **Tool Integration**

- Function calling with LLMs
- External API integration
- Tool result processing
- Slash command patterns

âœ… **Full-Stack AI Apps**

- Building REST APIs for AI services
- Creating interactive UIs
- Managing state and sessions
- Error handling and validation

âœ… **Best Practices**

- Modular code organization
- Environment configuration
- Type hints and Pydantic models
- Production-ready architecture

---

## ğŸ”§ Development

### Running Tests

```bash
uv run pytest
```

### Code Formatting

```bash
uv run ruff format .
uv run ruff check .
```

### Project Scripts

You can create convenience scripts:

```bash
# run_backend.sh
#!/bin/bash
uv run uvicorn app.main:app --reload --port 8000

# run_ui.sh
#!/bin/bash
uv run streamlit run app/ui.py
```

---

## ğŸ› ï¸ Configuration

All configuration is managed through environment variables in `.env`:

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENAI_API_KEY` | Your OpenAI API key | **Required** |
| `OPENAI_MODEL` | Model to use | `gpt-4o-mini` |
| `SQLITE_PATH` | Path to SQLite database | `.data/memory.sqlite` |
| `SYSTEM_PROMPT` | System prompt for the AI | `"You are a helpful and concise assistant."` |

---

## ğŸ¤” Common Questions

**Q: Why two memory modes?**  

A: Temporary mode is faster and useful for testing. Persistent mode is essential for production where conversations need to survive restarts.

**Q: Can I use a different LLM?**  

A: Yes! LangChain supports many providers (Anthropic, Azure, etc.). Just change the LLM initialization in `app/graph.py` and update `config.py`.

**Q: How do I add more tools?**  

A: Add new `@tool` decorated functions in `app/graph.py`, add them to the `TOOLS` list, and update the routing logic in `user_requested_tools()`.

**Q: Is this production-ready?**  

A: This is a solid foundation! For production, add authentication, rate limiting, logging, monitoring, and consider using PostgreSQL instead of SQLite for scale.

**Q: How does message trimming work?**  

A: The `trim_messages` utility keeps the most recent messages that fit within 1200 tokens, ensuring the context window isn't exceeded while preserving recent conversation context.

---

## ğŸ“š Additional Resources

- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [LangChain Documentation](https://python.langchain.com/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [OpenAI API Reference](https://platform.openai.com/docs/)

---

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details

---

## ğŸ™ Acknowledgments

Built with â¤ï¸ for students learning AI and conversational systems.

**Happy Learning! ğŸš€**

---

<div align="center">

**Made with** â¤ï¸ **for the AI learning community**

â­ **Star this repo if you found it helpful!**

[Project Link](https://github.com/JaimeLucena/langgraph-memory-chatbot)

</div>
